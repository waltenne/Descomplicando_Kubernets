# O que Ã© Container?

Um container Ã© uma unidade de software leve que empacota um aplicativo junto com todas as suas dependÃªncias, isolando-o do sistema operacional e de outros processos.

Ele nÃ£o isola apenas â€œrecursos como memÃ³ria e CPUâ€, mas isola o ambiente de execuÃ§Ã£o (bibliotecas, binÃ¡rios, filesystem e processos) usando recursos do prÃ³prio kernel, como:

- Namespaces â†’ isolamento de processos, rede, PID, filesystem, usuÃ¡rios
- cgroups â†’ controle de recursos (CPU, memÃ³ria, IO)

*â€œImagine que o computador fÃ­sico Ã© uma casa. Cada quarto Ã© um espaÃ§o isolado onde uma pessoa pode viver com seus prÃ³prios mÃ³veis e regras, mas todos os quartos compartilham a mesma estrutura da casa. Assim funcionam os containers: ambientes isolados que compartilham o mesmo kernel."

# O que Ã© Container Engine?


O Container Engine Ã© o componente que gerencia e organiza os containers, oferecendo comandos, APIs e funcionalidades prÃ¡ticas para o usuÃ¡rio.

Ele nÃ£o cria containers diretamente  ele orquestra.  
Quando vocÃª executa `docker run`, Ã© o engine (ex: Docker Engine ou Podman Engine) que recebe o comando e solicita ao Container Runtime que execute o container.

FunÃ§Ãµes principais:

- Gerenciar imagens    
- Criar e remover container=s (via runtime)    
- Criar redes e volumes    
- Gerenciar logs e eventos    
- Executar health checks    
- Fornecer CLI e API REST    
- Controlar o ciclo de vida dos containers

*"Se o container Ã© um quarto e o runtime Ã© o pedreiro que o constrÃ³i, o Container Engine Ã© o â€œarquiteto/gerenteâ€ que organiza tudo: ele decide onde o quarto serÃ¡ feito, define as regras, registra quem estÃ¡ usando qual quarto e chama o pedreiro para construir quando necessÃ¡rio."*

# O que Ã© Container Runtime?


O Container Runtime Ã© o componente responsÃ¡vel por realmente _executar_ o container, interagindo diretamente com o kernel do sistema operacional.

Ele usa mecanismos do kernel  namespaces, cgroups, montagem de filesystem, isolamento de rede  para criar o ambiente isolado onde o processo vai rodar.  
Ou seja: ele Ã© quem transforma a definiÃ§Ã£o do container (imagem + configs) em um processo real isolado.

Existem dois nÃ­veis:

- Low-level runtimes (ex: runc, crun) â†’ falam diretamente com o kernel para criar o container    
- High-level runtimes (ex: containerd, cri-o) â†’ gerenciam mÃºltiplos containers e chamam o runtime low-level    

FunÃ§Ãµes principais:

- Criar o processo isolado do container    
- Configurar namespaces e cgroups    
- Montar o filesystem    
- Lidar com syscalls do kernel    
- Iniciar, parar e remover containers em nÃ­vel de sistema operacional
    
*"Se o computador Ã© uma casa e o container Ã© um quarto, o Container Runtime Ã© o â€œpedreiroâ€ que realmente constrÃ³i o quarto  levantando as paredes, instalando a porta, conectando energia e deixando tudo isolado.  
Ele Ã© quem faz o trabalho fÃ­sico usando a estrutura da casa (kernel)."*

# O que Ã© OCI (Open Container Iniciative)?

A OCI  Open Container Initiative  Ã© uma organizaÃ§Ã£o que define padrÃµes abertos para containers.  
Ela nÃ£o executa containers, nÃ£o cria imagens e nÃ£o roda nada.

A funÃ§Ã£o da OCI Ã© garantir que todos os containers e runtimes sigam o mesmo conjunto de regras, permitindo interoperabilidade entre ferramentas diferentes (Docker, Podman, containerd, CRI-O, Kubernetes, etc.).

A OCI define dois padrÃµes principais:

1. OCI Image Specification  
    â†’ Define como uma imagem de container deve ser estruturada (camadas, manifestos, configuraÃ§Ã£o).  
    Isso garante que uma imagem criada no Docker pode ser usada no Podman, containerd ou Kubernetes sem problemas.
    
2. OCI Runtime Specification  
    â†’ Define como um container deve ser executado:
    
    - namespaces        
    - cgroups        
    - montagem de filesystem        
    - formato do `config.json`  
        Isso garante que runtimes como runc e crun executem containers de forma compatÃ­vel.
        

A OCI traz padronizaÃ§Ã£o, evitando que cada fornecedor invente seu prÃ³prio formato de imagem.

## Por que a OCI existe?

Antes da OCI, cada ferramenta fazia containers â€œdo seu jeitoâ€.  
Isso criava incompatibilidade entre imagens e runtimes.

Criar um container no Docker e rodar no Kubernetes, por exemplo, era um desafio.

Hoje:

- Docker cria imagens compatÃ­veis com OCI    
- Podman cria imagens compatÃ­veis com OCI    
- containerd, CRI-O, Kubernetes e Cloud Providers usam runtimes compatÃ­veis com OCI

_Se o computador Ã© uma casa e containers sÃ£o quartos:_

- A OCI Ã© a empresa que escreveu as normas de construÃ§Ã£o.    
- Ela diz:
    
    - como as portas devem ser,        
    - como as paredes devem ser montadas,        
    - quais medidas mÃ­nimas sÃ£o necessÃ¡rias,        
    - como os quartos devem ser organizados.

_O objetivo Ã© garantir que qualquer pedreiro (runtime) possa construir um quarto e que qualquer gerente/arquiteto (engine) consiga trabalhar com esse padrÃ£o sem precisar reinventar tudo._

> A OCI Ã© responsÃ¡vel por definir os padrÃµes oficiais que garantem que containers e imagens funcionem da mesma forma em qualquer ferramenta.  
> Ela padroniza formatos e comportamentos, permitindo compatibilidade entre Docker, Podman, containerd, CRI-O e Kubernetes.

# O que Ã© Kubernets?


Kubernetes Ã© uma plataforma de orquestraÃ§Ã£o de containers responsÃ¡vel por gerenciar centenas ou milhares de containers de forma automÃ¡tica, eficiente e resiliente.

Ele resolve problemas que aparecem quando vocÃª executa containers em larga escala, como:

- Onde cada container deve rodar?    
- O que fazer se um container ou mÃ¡quina falhar?    
- Como distribuir carga entre containers?    
- Como atualizar containers sem derrubar o sistema?    
- Como garantir que sempre existam â€œXâ€ rÃ©plicas rodando?    

Kubernetes automatiza tudo isso.

## O que exatamente o Kubernetes faz?

1. Orquestra containers

Decide onde containers vÃ£o rodar dentro de um cluster.

2. MantÃ©m aplicaÃ§Ãµes sempre funcionando

Se um container morre, ele cria outro automaticamente.

3. Escala aplicaÃ§Ãµes

- para cima (mais rÃ©plicas)    
- para baixo (menos rÃ©plicas)    

de acordo com demanda, mÃ©tricas ou regras.

 4. Faz atualizaÃ§Ãµes sem downtime

Implementa:

- rolling updates    
- rollbacks    
- versionamento de deployments

5. Gerencia rede e comunicaÃ§Ã£o

- Service Discovery    
- Load Balancing    
- IP fixo para serviÃ§os    

6. Isola e organiza aplicaÃ§Ãµes

Usa objetos como:

- Pods    
- Deployments    
- Services    
- Namespaces- 

Kubernetes NÃƒO Ã©â€¦

- nÃ£o executa containers    
- nÃ£o constrÃ³i imagens    
- nÃ£o substitui o Container Engine    
- nÃ£o substitui o Container Runtime    

Ele apenas orquestra os componentes abaixo dele.---

_Se o computador Ã© uma casa, containers sÃ£o quartos, o runtime Ã© o pedreiro e o engine Ã© o arquiteto/gerenteâ€¦ entÃ£o:_

Kubernetes Ã© o sÃ­ndico de um condomÃ­nio gigantesco.

- Ele decide onde cada quarto (container) serÃ¡ construÃ­do    
- Garante que sempre haja o nÃºmero certo de quartos funcionando    
- Se um quarto pega fogo, ele manda construir outro imediatamente    
- Distribui moradores entre quartos conforme a demanda    
- Organiza redes, regras, acessos, manutenÃ§Ã£o e atualizaÃ§Ãµes    
- Garante que o condomÃ­nio continue funcionando mesmo se um bloco inteiro cair    


> Kubernetes Ã© uma plataforma que automatiza o deployment, escalonamento, atualizaÃ§Ã£o e recuperaÃ§Ã£o de containers em um cluster de servidores.  
> Ele garante que aplicaÃ§Ãµes rodem de forma estÃ¡vel, distribuÃ­da e resiliente, mesmo em larga escala.

# O que sao Workers e o Control Plane do Kubernets?

Dentro de um cluster Kubernetes, existem dois grandes grupos de mÃ¡quinas (nÃ³s):

- Control Plane â†’ o â€œcÃ©rebroâ€ do cluster    
- Worker Nodes â†’ os â€œtrabalhadoresâ€ onde os containers realmente rodam    

Eles tÃªm papÃ©is totalmente diferentes, mas funcionam juntos para manter o cluster estÃ¡vel.

## O que Ã© o Control Plane?

O Control Plane Ã© o conjunto de componentes responsÃ¡veis por controlar, gerenciar e decidir tudo no Kubernetes.  
Ele Ã© literalmente o cÃ©rebro e a camada de orquestraÃ§Ã£o.

Ã‰ formado por vÃ¡rios serviÃ§os crÃ­ticos:

### Componentes do Control Plane

- etcd â†’ banco de dados distribuÃ­do que armazena o estado desejado do cluster   
- kube-apiserver â†’ a â€œporta de entradaâ€ do cluster; recebe todos os comandos
- kube-scheduler â†’ decide em qual Worker cada Pod deve rodar    
- kube-controller-manager â†’ cria, replica e mantÃ©m tudo funcionando    
- cloud-controller-manager â†’ integra com provedores de nuvem (quando existe)    

### FunÃ§Ãµes principais

- Decide onde Pods serÃ£o criados    
- Monitora o cluster    
- Reage a falhas e recria recursos    
- MantÃ©m o â€œestado desejadoâ€    
- Atua como cÃ©rebro, juiz e gerente do cluster    

> O Control Plane nÃ£o roda aplicaÃ§Ãµes do usuÃ¡rio ele sÃ³ gerencia.

## O que sÃ£o os Worker Nodes?

Workers sÃ£o as mÃ¡quinas onde os containers realmente rodam.  
Eles fazem o trabalho pesado.

### Componentes principais do Worker

- kubelet â†’ agente que recebe ordens do Control Plane    
- kube-proxy â†’ controla a rede e load balancing interno    
- Container Runtime (ex: containerd, CRI-O) â†’ executa os containers    

### FunÃ§Ãµes principais

- Rodar Pods (containers)    
- Criar e parar containers sob ordem do controlar plane    
- Gerenciar recursos locais (CPU, memÃ³ria, storage)    
- Enviar status para o Control Plane    
- Implementar networking interno    

> Workers sÃ£o as mÃ¡quinas onde as aplicaÃ§Ãµes realmente vivem.

- O cluster Kubernetes Ã© um condomÃ­nio inteiro    
- Cada container Ã© um quarto dentro de um apartamento    
- Os Workers sÃ£o os blocos de apartamentos onde os moradores vivem  
    â†’ SÃ£o eles que possuem os quartos (containers) de verdade    
- O Control Plane Ã© a administradora/sÃ­ndico do condomÃ­nio, que:    
    - decide onde cada morador vai ficar        
    - fiscaliza tudo        
    - distribui tarefas        
    - lida com problemas e falhas        
    - garante que o condomÃ­nio estÃ¡ funcionando       
    
- Control Plane = sÃ­ndico/administrador do condomÃ­nio (orquestra, decide, controla)*
- Workers = blocos de apartamentos com os quartos onde as pessoas realmente moram (executam containers)

> O Control Plane decide, gerencia e orquestra.  
> Os Workers executam os containers.  
> O Control Plane Ã© o cÃ©rebro.  
> Os Workers sÃ£o os mÃºsculos.

# Quais os componentes do Workers do Kubernets?

O Worker Node Ã© a mÃ¡quina dentro do cluster Kubernetes onde os Pods e containers realmente rodam.  
Para que isso aconteÃ§a, ele possui trÃªs componentes essenciais:

## ğŸ”§ 1. Kubelet (agente do Kubernetes no nÃ³)

Ã‰ o principal agente do Worker Node.  
Ele se comunica com o Control Plane e garante que tudo o que foi solicitado realmente estÃ¡ sendo executado no nÃ³.

### FunÃ§Ãµes do kubelet:

- Recebe ordens do Control Plane (API Server)    
- Cria, inicia e monitora Pods    
- Envia status do nÃ³ e dos Pods para o Control Plane    
- Garante que o â€œestado desejadoâ€ esteja sempre sendo seguido    
- Interage com o Container Runtime para criar containers    
- Realiza health checks   

Sem o kubelet, o Worker nÃ£o participa do cluster.

## ğŸ”Œ 2. Kube-proxy (gerencia rede dentro do nÃ³)

O kube-proxy cuida do trÃ¡fego de rede dentro e fora do nÃ³.

### FunÃ§Ãµes do kube-proxy:

- Implementa regras de roteamento de rede    
- Garante comunicaÃ§Ã£o entre Pods    
- Implementa o load balancing interno dos Services    
- Configura regras de iptables / eBPF    
- Gerencia portas e acesso aos serviÃ§os dentro do cluster
## ğŸ¬ 3. Container Runtime (executa os containers)

Ã‰ o componente que roda os containers de verdade.

Exemplos mais comuns:

- containerd (o mais usado hoje)    
- CRI-O (usado muito em clusters Kubernetes puros)    
- Docker Engine (cada vez menos usado â€” desde 1.20 nÃ£o Ã© nativo)    
- runc / crun (low-level runtimes usados por containerd/CRI-O)    

### FunÃ§Ãµes do Container Runtime:

- Criar containers    
- Isolar processos (namespaces, cgroups)    
- Configurar filesystem    
- Baixar e armazenar imagens    
- Gerenciar ciclo de vida dos containers    

Sem um runtime, nenhum container seria executado.

Dependendo da instalaÃ§Ã£o, o Worker pode ter:

- CNI Plugins (Container Network Interface)  
    â†’ Usados para redes de Pods (Calico, Flannel, Cilium etc.)    
- CSI Plugins (Container Storage Interface)  
    â†’ Gerenciam volumes e storage (EBS, Ceph, Longhorn, NFS etc.)    

Esses nÃ£o sÃ£o â€œcomponentes nativosâ€, mas fazem parte do funcionamento do cluster.

Em um condomÃ­nio (cluster):

- Worker Node = bloco de apartamentos    
- Kubelet = zelador do bloco    
    - garante que os quartos (Pods) estejam corretos        
    - reporta status ao sÃ­ndico (Control Plane)        
- Kube-proxy = porteiro do bloco*    
    - controla o trÃ¡fego de entrada/saÃ­da        
    - direciona visitantes e moradores (rede e regras de acesso)        
- Container Runtime = construtor    
    - cria de verdade os quartos (containers) dentro dos apartamentos (Pods)        


Componentes essenciais do Worker Node:

1. kubelet â†’ agente que recebe ordens e gerencia Pods    
2. kube-proxy â†’ gerencia rede e load balancing    
3. container runtime â†’ executa containers    

Com isso, o Worker Node pode executar aplicaÃ§Ãµes dentro do cluster.

# Quais as portas TCP e UDP dos componentes do Kubernets?

O Kubernetes usa vÃ¡rias portas para permitir comunicaÃ§Ã£o entre o Control Plane, os Workers, e os componentes internos como kubelet, kube-apiserver, etcd, kube-proxy, runtime, etc.

Abaixo estÃ£o todas as portas oficiais, divididas por grupo.
## ğŸ§  1. Portas do Control Plane

### ğŸ“Œ kube-apiserver (API Server) â€” TCP

Portas obrigatÃ³rias:

|Porta|Protocolo|FunÃ§Ã£o|
|---|---|---|
|6443|TCP|Porta principal da API Kubernetes|
|8080|TCP|API HTTP sem TLS (desativada na maioria das instalaÃ§Ãµes modernas)|

### ğŸ“Œ etcd â€” banco de dados do Kubernetes â€” TCP

| Porta    | Protocolo | FunÃ§Ã£o                                       |
| -------- | --------- | -------------------------------------------- |
| 2379 | TCP       | ComunicaÃ§Ã£o dos clientes (API Server â†’ etcd) |
| 2380 | TCP       | ComunicaÃ§Ã£o entre membros do cluster etcd    |
|          |           |                                              |

### ğŸ“Œ kube-scheduler â€” TCP

|Porta|Protocolo|FunÃ§Ã£o|
|---|---|---|
|10259|TCP|Porta segura do kube-scheduler (TLS)|

### ğŸ“Œ kube-controller-manager â€” TCP

|Porta|Protocolo|FunÃ§Ã£o|
|---|---|---|
|10257|TCP|Porta segura do controller-manager (TLS)|

## âš™ï¸ 2. Portas dos Worker Nodes

### ğŸ“Œ kubelet â€” TCP

| Porta     | Protocolo | FunÃ§Ã£o                                                          |
| --------- | --------- | --------------------------------------------------------------- |
| 10250 | TCP       | Porta principal do kubelet (API interna)                        |
| 10255     | TCP       | Porta de leitura (desativada em versÃµes recentes por seguranÃ§a) |

### ğŸ“Œ kube-proxy â€” TCP/UDP

O kube-proxy usa portas dinÃ¢micas, dependendo do modo (iptables, ipvs ou eBPF).  
PorÃ©m, ele normalmente:

- NÃ£o expÃµe portas fixas,    
- Mas usa portas do range definido pelos Services do cluster.    
### Range padrÃ£o dos Services:

|Range|Protocolo|FunÃ§Ã£o|
|---|---|---|
|30000â€“32767|TCP/UDP|NodePorts usados para acessar Services externamente|


## ğŸ§© 3. Portas usadas pelo Cluster como um todo

### ğŸ“Œ NodePort Range â€” TCP/UDP

Portas abertas em todos os Workers quando um Service usa NodePort:

| Range           | Protocolo | FunÃ§Ã£o                      |
| --------------- | --------- | --------------------------- |
| 30000â€“32767 | TCP/UDP   | Acesso externo aos Services |

### ğŸ“Œ CNI Plugins (rede)

Dependem do plugin (Calico, Flannel, Cilium etc.). Exemplos:

### Calico

|Porta|Protocolo|FunÃ§Ã£o|
|---|---|---|
|179|TCP|BGP peering|
|4789|UDP|VXLAN|
|5473|TCP|Typha|

### Flannel

|Porta|Protocolo|
|---|---|
|8472|UDP (VXLAN)|

### Cilium

|Porta|Protocolo|
|---|---|
|4240|TCP|
|8472|UDP|

## ğŸ§  4. ComunicaÃ§Ã£o entre componentes (importante em firewalls)

### Control Plane â†’ Worker

|Porta|Protocolo|FunÃ§Ã£o|
|---|---|---|
|10250|TCP|kubelet API|
|30000â€“32767|TCP/UDP|NodePorts|

### Worker â†’ Control Plane

|Porta|Protocolo|FunÃ§Ã£o|
|---|---|---|
|6443|TCP|API Server|

### Worker â†” Worker

Depende do plugin de rede (CNI), geralmente UDP (VXLAN) ou TCP (BGP/eBPF).


- API Server (6443) Ã© a portaria principal do condomÃ­nio.    
- etcd (2379-2380) Ã© o arquivo central onde tudo Ã© registrado.    
- kubelet (10250) Ã© o telefone do zelador do bloco, onde ele recebe ordens.    
- NodePorts (30000â€“32767) sÃ£o as garagens numeradas que permitem acesso externo ao bloco.    
- CNI (como Calico/Flannel) sÃ£o as ruas internas, com suas prÃ³prias regras de trÃ¡fego.

 ğŸ”¹ Control Plane

- 6443/TCP â€” API Server    
- 2379â€“2380/TCP â€” etcd    
- 10257/TCP â€” Controller Manager    
- 10259/TCP â€” Scheduler    

ğŸ”¹ Workers

- 10250/TCP â€” kubelet    
- 30000â€“32767/TCP/UDP â€” NodePorts    
- (CNI pode usar 4789/UDP, 8472/UDP, 179/TCP, etc.)

# IntroduÃ§Ã£o Pods, Replica Set, Deployments e Service

Um Pod Ã© a menor unidade que o Kubernetes gerencia.  
Ele Ã© um invÃ³lucro que contÃ©m um ou mais containers que devem rodar juntos e compartilhar:

- rede    
- IP    
- portas    
- filesystem temporÃ¡rio   

> Na prÃ¡tica, a maioria dos Pods contÃ©m apenas 1 container, mas podem ter mais (ex: sidecar containers).

FunÃ§Ãµes do Pod:

- Executar containers    
- Compartilhar recursos entre os containers internos    
- Ser a â€œunidade bÃ¡sicaâ€ de scheduling

## Deployment gerencia versÃµes, atualizaÃ§Ãµes e ReplicaSets

O Deployment Ã© o nÃ­vel mais alto de controle para aplicaÃ§Ãµes â€œstatelessâ€.  
Ele usa ReplicaSets internamente para gerenciar Pods.

### O que um Deployment oferece:

- Deploy de uma aplicaÃ§Ã£o    
- AtualizaÃ§Ãµes sem downtime (rolling updates)    
- Rollbacks automÃ¡ticos se algo der errado    
- HistÃ³rico de versÃµes    
- Controle declarativo do estado (YAML)    

Quando vocÃª altera a versÃ£o da imagem, o Deployment:

1. Cria um novo ReplicaSet    
2. Diminui o antigo ReplicaSet    
3. Faz troca gradual    
4. MantÃ©m o sistema disponÃ­vel   

> Ã‰ o jeito padrÃ£o e recomendado de rodar aplicaÃ§Ãµes no Kubernetes.

## ReplicaSet garante o nÃºmero de Pods em execuÃ§Ã£o

Um ReplicaSet Ã© o componente responsÃ¡vel por garantir que sempre exista a quantidade desejada de Pods rodando.

Exemplo:

- VocÃª diz que quer 3 rÃ©plicas    
- Se 1 Pod morrer, o ReplicaSet cria outro    
- Se houver 4 Pods, ele apaga 1   

> PorÃ©m: ReplicaSets raramente sÃ£o usados diretamente, porque o Deployment faz esse trabalho para vocÃª.

## Service â€” fornece networking e acesso estÃ¡vel aos Pods

Pods nascem e morrem o tempo todo.  
Cada vez que um Pod Ã© recriado, ele ganha um novo IP.

O Service resolve esse problema, oferecendo:

- Um IP fixo e estÃ¡vel    
- Load balancing entre Pods    
- Descoberta de serviÃ§o    
- Regras de rede (ClusterIP, NodePort, LoadBalancer)    

### Tipos mais usados:

- ClusterIP â†’ acesso interno (padrÃ£o)    
- NodePort â†’ expÃµe porta em todos os Workers    
- LoadBalancer â†’ cria load balancer do provedor cloud    
- Headless Service â†’ para acesso direto aos Pods (StatefulSets)    

> Sem um Service, sua aplicaÃ§Ã£o nÃ£o seria acessÃ­vel de forma confiÃ¡vel.

- Pod â†’ Ã© um apartamento (contÃ©m um ou mais moradores = containers).    
- ReplicaSet â†’ garante que sempre existam x apartamentos ocupados.    
- Deployment â†’ Ã© o administrador do bloco, que controla reformas, atualizaÃ§Ãµes e histÃ³rico.    
- Service â†’ Ã© a portaria/telefone com ramal fixo que sempre sabe onde cada morador estÃ¡, mesmo que se mudem de apartamento (Pods morram e sejam recriados).

|Componente|FunÃ§Ã£o|
|---|---|
|Pod|Executa containers; menor unidade do Kubernetes|
|ReplicaSet|MantÃ©m o nÃºmero desejado de Pods|
|Deployment|Gerencia deploys, updates e ReplicaSets|
|Service|ExpÃµe e dÃ¡ acesso estÃ¡vel aos Pods|

# Entendimento e Instalado o kubectl


O kubectl Ã© a ferramenta de linha de comando usada para interagir com o Kubernetes.  
Ã‰ atravÃ©s dele que vocÃª cria, consulta, atualiza e exclui recursos dentro de um cluster.

Com o kubectl vocÃª pode, por exemplo:

- Criar Pods, Deployments e Services    
- Ver logs dos containers    
- Aplicar arquivos YAML    
- Executar comandos dentro de containers    
- Ver o estado do cluster    
- Escalar aplicaÃ§Ãµes    

Ou seja: kubectl Ã© o â€œcontrole remotoâ€ do Kubernetes.


Pense no Kubernetes como um grande condomÃ­nio:

- O Control Plane Ã© a administraÃ§Ã£o do condomÃ­nio.    
- Os Workers sÃ£o os apartamentos onde o trabalho realmente acontece.    
- O kubectl Ã© o interfone:  
    Ã‰ por meio dele que vocÃª se comunica com a administraÃ§Ã£o (API Server), envia ordens e recebe informaÃ§Ãµes.

## âœ” Como instalar o kubectl

A instalaÃ§Ã£o varia de acordo com o sistema operacional. Aqui estÃ£o as formas mais comuns (modo resumido e moderno):

 ğŸ”µ Linux (qualquer distro)

```bash
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
chmod +x kubectl
sudo mv kubectl /usr/local/bin/
```

Verificar versÃ£o:

```bash
kubectl version --client
```

---

ğŸŸ£ Ubuntu/Debian (via apt)

```bash
sudo apt-get update
sudo apt-get install -y ca-certificates curl
sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg \
  https://packages.kubernetes.io/core:/stable:/v1.29/deb/Release.key
echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] \
  https://packages.kubernetes.io/core:/stable:/v1.29/deb/ /" \
  | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt-get update
sudo apt-get install -y kubectl
```

---

 ğŸŸ¦ Windows (via Chocolatey)

```powershell
choco install kubernetes-cli
```

---

 ğŸŸ© Windows (via Scoop)

```powershell
scoop install kubectl
```

---

 ğŸŸ§ MacOS (via Homebrew)

```bash
brew install kubectl
```


## ğŸ“Œ Como testar se estÃ¡ funcionando

Depois de conectado a um cluster (ex.: kind, minikube, k3d, EKS, GKE etc.):

```bash
kubectl get nodes
```

# Criando o nosso primeiro Cluster com o Kind

O Kind (Kubernetes IN Docker) Ã© uma ferramenta que permite criar clusters Kubernetes locais usando containers Docker como nodes.  
Ele Ã© leve, rÃ¡pido e ideal para:

- estudos    
- testes locais    
- CI/CD    
- desenvolvimento   

Em vez de criar VMs ou usar cloud, o Kind faz tudo dentro de containers Docker.

Se o computador Ã© uma casa:

- O Docker Ã© como construir â€œquartos rÃ¡pidosâ€ usando paredes de drywall.    
- O Kind usa esses quartos para montar um mini-condomÃ­nio Kubernetes completo dentro da sua casa.   

Ou seja:  
â†’ Cada node do cluster Kind Ã© um container Docker.

## âœ” 1. PrÃ©-requisitos

- Docker instalado e funcionando    
- Linux, macOS ou Windows (via WSL2 ou Docker Desktop)    

Testar Docker:

```bash
docker ps
```
## âœ” 2. Instalando o Kind

### Linux / macOS

```bash
curl -Lo ./kind https://kind.sigs.k8s.io/dl/latest/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
```

### Windows (PowerShell)

```powershell
choco install kind
```

Verificar:

```bash
kind version
```
## âœ” 3. Criando o Primeiro Cluster

O comando padrÃ£o Ã©:

```bash
kind create cluster
```

Isso cria:

- 1 node de controle    
- kubeconfig para o kubectl    
- comunicaÃ§Ã£o com API Server    
- tudo isolado dentro do Docker    

Testar:

```bash
kubectl get nodes
```

VocÃª deve ver algo como:

```
NAME                 STATUS   ROLES           VERSION
kind-control-plane   Ready    control-plane   v1.29.x
```

## âœ” 4. Criando Cluster com ConfiguraÃ§Ã£o Personalizada

VocÃª pode criar clusters com mÃºltiplos nodes:

Crie o arquivo `cluster.yaml`:

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
```

Criar:

```bash
kind create cluster --config cluster.yaml --name giropops
```

Agora:

```bash
kubectl get nodes
```

```
NAME                       ROLES           STATUS
giropops-control-plane     control-plane    Ready
giropops-worker            worker           Ready
giropops-worker2           worker           Ready
```
## âœ” 5. Deletando o Cluster

```bash
kind delete cluster
```

## âœ” 6. Onde fica o kubeconfig?

O Kind escreve automaticamente no arquivo padrÃ£o:

```
~/.kube/config
```

Ou seja: kubectl jÃ¡ fica pronto para uso.

O Kind Ã© a forma mais rÃ¡pida e simples de criar um ambiente Kubernetes local.  
Ele usa containers como nodes e permite criar clusters completos para estudo e desenvolvimento em apenas alguns segundos.

Aqui estÃ¡ o texto ajustado, com explicaÃ§Ã£o clara sobre namespaces, comandos Ãºteis, boas prÃ¡ticas e mantendo o mesmo estilo dos seus outros tÃ³picos:


# Primeiros passos no Kubernetes com o kubectl

Agora que vocÃª jÃ¡ tem um cluster (ex.: Kind, Minikube, k3d ou cloud) e o kubectl instalado, Ã© hora de dar os primeiros comandos essenciais para entender como conversar com o Kubernetes.

O kubectl Ã© o seu controle remoto â€” tudo o que vocÃª quer que o cluster faÃ§a, vocÃª pede atravÃ©s dele.


## ğŸ§­ 1. Verificando o estado do cluster

### âœ” Listar os nodes

```bash
kubectl get nodes
```

### âœ” Ver informaÃ§Ãµes detalhadas

```bash
kubectl describe nodes
```

## ğŸ—‚ï¸ 2. Entendendo Namespaces

Um Namespace Ã© uma forma de organizar recursos dentro do cluster.  
Eles servem para separar ambientes, isolar times, ou simplesmente organizar melhor os objetos.

Namespaces mais comuns:

- `default` â†’ onde seus recursos vÃ£o por padrÃ£o    
- `kube-system` â†’ onde ficam componentes internos    
- `kube-public` â†’ acesso pÃºblico    
- `kube-node-lease` â†’ health-check dos nodes    
- `dev`, `hml`, `prod` â†’ comuns em clusters de empresas    

### âœ” Listar namespaces:

```bash
kubectl get ns
```

### âœ” Criar um namespace:

```bash
kubectl create ns meu-ambiente
```

### âœ” Usar comandos em um namespace especÃ­fico:

```bash
kubectl get pods -n meu-ambiente
```

### âœ” Definir permanentemente o namespace em um contexto:

```bash
kubectl config set-context --current --namespace=meu-ambiente
```

Dica: se vocÃª nÃ£o especificar um namespace, o kubectl usa o `default`.


## ğŸ“¦ 3. Criando o seu primeiro Pod

Vamos criar um Pod simples rodando Nginx:

```bash
kubectl run meu-pod --image=nginx
```

Verificar:

```bash
kubectl get pods
```

Ver detalhes:

```bash
kubectl describe pod meu-pod
```

Ver logs:

```bash
kubectl logs meu-pod
```

Entrar dentro do container:

```bash
kubectl exec -it meu-pod -- bash
```

## ğŸ”„ 4. Deletando um Pod

```bash
kubectl delete pod meu-pod
```

## ğŸ“ 5. Criando recursos via YAML

Crie um arquivo `pod.yaml`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: meu-pod
spec:
  containers:
    - name: nginx
      image: nginx
```

Aplicar:

```bash
kubectl apply -f pod.yaml
```

Ver mudanÃ§as:

```bash
kubectl get pods
```

Atualizar e reaplicar:

```bash
kubectl apply -f pod.yaml
```

## ğŸŒ 6. Expondo o Pod com um Service

```bash
kubectl expose pod meu-pod --port=80 --type=NodePort
```

Ver service:

```bash
kubectl get svc
```

## ğŸ§¹ 7. Limpando recursos

```bash
kubectl delete -f pod.yaml
kubectl delete svc meu-pod
```


## ğŸ§  Dicas importantes iniciantes

### âœ” DocumentaÃ§Ã£o interna do Kubernetes (SUPER Ãºtil)

```bash
kubectl explain pod
kubectl explain deployment.spec.replicas
```

### âœ” Ver tudo o que estÃ¡ rodando no cluster:

```bash
kubectl get all --all-namespaces
```

### âœ” Ver recursos separados por namespace:

```bash
kubectl get pods -n kube-system
```

### âœ” Auto-complete do kubectl (altamente recomendado)

```bash
source <(kubectl completion bash)
```

- O cluster Ã© o condomÃ­nio    
- Cada namespace Ã© como um _andar_ ou _bloco_ dentro do condomÃ­nio    
- Os Workers sÃ£o os apartamentos    
- O kubelet Ã© o zelador    
- O kubectl Ã© o interfone que vocÃª usa para dar ordens e consultar o estado de tudo


Com esses comandos vocÃª jÃ¡ sabe:

- usar namespaces    
- listar recursos    
- criar Pods    
- visualizar logs    
- acessar containers    
- aplicar arquivos YAML    
- criar Services    
- gerenciar o cluster com kubectl    

# Conhecendo o YAML e o kubectl com dry-run

O Kubernetes utiliza arquivos YAML para definir tudo o que existe dentro do cluster: Pods, Deployments, Services, ConfigMaps, Secrets e muito mais.  
O YAML funciona como um â€œmanual de instruÃ§Ãµesâ€ que diz ao Kubernetes exatamente como vocÃª quer que um recurso seja criado.

JÃ¡ o comando dry-run permite testar a criaÃ§Ã£o de recursos sem realmente criÃ¡-los, ajudando vocÃª a validar YAMLs e gerar arquivos de configuraÃ§Ã£o automaticamente.

## ğŸ“˜ 1. O que Ã© YAML no Kubernetes?

O YAML Ã© um formato baseado em indentaÃ§Ã£o que descreve objetos.  
Ele segue sempre a mesma estrutura:

```
apiVersion
kind
metadata
spec
```

### âœ” Estrutura bÃ¡sica

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: meu-pod
spec:
  containers:
    - name: nginx
      image: nginx
```

### âœ” ExplicaÃ§Ã£o simples:

- apiVersion â†’ qual versÃ£o da API o recurso usa    
- kind â†’ tipo do objeto (Pod, Deployment, Serviceâ€¦)    
- metadata â†’ nome, labels, anotaÃ§Ãµes    
- spec â†’ a â€œreceitaâ€ de como o objeto funciona


No nosso condomÃ­nio Kubernetes:

- O YAML Ã© o projeto arquitetÃ´nico que vocÃª entrega para a administraÃ§Ã£o.    
- O Kubernetes lÃª esse projeto e constrÃ³i o que vocÃª pediu.    
- Mudar o YAML = pedir uma reforma.

## ğŸ“„ 2. Criando YAMLs automaticamente com kubectl (dry-run)

O kubectl pode gerar automaticamente arquivos YAML para vocÃª usando o dry-run, evitando que vocÃª tenha que escrever tudo do zero.

### âœ” Exemplo: gerar YAML de um Pod sem criar nada

```bash
kubectl run meu-pod --image=nginx --dry-run=client -o yaml
```

SaÃ­da: um YAML completo do Pod.

### âœ” Exportar para um arquivo

```bash
kubectl run meu-pod --image=nginx --dry-run=client -o yaml > pod.yaml
```

Agora vocÃª pode editar o arquivo e depois aplicar:

```bash
kubectl apply -f pod.yaml
```
## ğŸ§ª 3. O que Ã© kubectl dry-run?

O dry-run permite testar aÃ§Ãµes sem realmente executÃ¡-las.

Existem dois tipos:

### âœ” `--dry-run=client`

O kubectl valida o YAML localmente, sem falar com o cluster.  
Bom para gerar arquivos.

### âœ” `--dry-run=server`

O kubectl envia a configuraÃ§Ã£o para o cluster, mas nÃ£o cria o recurso.  
O servidor valida tudo (versÃµes, campos, API, policies).

Exemplo:

```bash
kubectl apply -f pod.yaml --dry-run=server
```

Se houver erro no YAML, ele avisa antes de vocÃª aplicar de verdade.


## ğŸ”§ 4. Gerando YAMLs para outros recursos

### âœ” Deployment

```bash
kubectl create deployment meu-app \
  --image=nginx \
  --replicas=3 \
  --dry-run=client -o yaml
```

### âœ” Service

```bash
kubectl expose deployment meu-app \
  --port=80 \
  --type=NodePort \
  --dry-run=client -o yaml
```

### âœ” Namespace

```bash
kubectl create ns dev --dry-run=client -o yaml
```

## ğŸ” 5. Validando e Inspecionando YAML

### âœ” Mostrar documentaÃ§Ã£o de qualquer campo:

```bash
kubectl explain pod.spec
```

Campo especÃ­fico:

```bash
kubectl explain deployment.spec.strategy
```

### âœ” Validar YAML sem criar:

```bash
kubectl apply -f pod.yaml --dry-run=server
```

Com YAML e o `dry-run`, vocÃª aprende:

- Como o Kubernetes representa e cria recursos    
- Como gerar YAML automaticamente    
- Como validar configuraÃ§Ãµes antes de aplicÃ¡-las    
- Como evitar erros antes de impactar o cluster    
- Como usar o kubectl de maneira prÃ¡tica e profissional    